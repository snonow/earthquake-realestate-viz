{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86acf7a-a359-43b4-8ca1-d8c6791a0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45857841-7274-4700-b0e4-3ab74c6b41fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           time                                 place    status  tsunami  \\\n",
      "0  631153353990     12 km NNW of Meadow Lakes, Alaska  reviewed        0   \n",
      "1  631153491210            14 km S of Volcano, Hawaii  reviewed        0   \n",
      "2  631154083450            7 km W of Cobb, California  reviewed        0   \n",
      "3  631155512130  11 km E of Mammoth Lakes, California  reviewed        0   \n",
      "4  631155824490                16km N of Fillmore, CA  reviewed        0   \n",
      "\n",
      "   significance   data_type  magnitudo       state   longitude   latitude  \\\n",
      "0            96  earthquake       2.50      Alaska -149.669200  61.730200   \n",
      "1            31  earthquake       1.41      Hawaii -155.212333  19.317667   \n",
      "2            19  earthquake       1.11  California -122.806167  38.821000   \n",
      "3            15  earthquake       0.98  California -118.846333  37.664333   \n",
      "4           134  earthquake       2.95  California -118.934000  34.546000   \n",
      "\n",
      "    depth                              date  \n",
      "0  30.100  1990-01-01 00:22:33.990000+00:00  \n",
      "1   6.585  1990-01-01 00:24:51.210000+00:00  \n",
      "2   3.220  1990-01-01 00:34:43.450000+00:00  \n",
      "3  -0.584  1990-01-01 00:58:32.130000+00:00  \n",
      "4  16.122  1990-01-01 01:03:44.490000+00:00  \n",
      "      brokered_by    status     price  bed  bath  acre_lot     street    city  \\\n",
      "3409      21163.0  for_sale  525000.0  3.0   3.0      0.45  1813270.0  Agawam   \n",
      "3410      67455.0  for_sale  289900.0  3.0   2.0      0.36  1698080.0  Agawam   \n",
      "3416      97400.0  for_sale  384900.0  3.0   2.0      0.46  1244899.0  Agawam   \n",
      "3423      33714.0  for_sale  199999.0  3.0   2.0      1.76  1745924.0  Agawam   \n",
      "3430      22188.0  for_sale  419000.0  4.0   2.0      2.00  1417448.0  Pelham   \n",
      "\n",
      "              state  zip_code  house_size prev_sold_date  \n",
      "3409  Massachusetts    1001.0      2314.0     2014-06-25  \n",
      "3410  Massachusetts    1001.0      1276.0     2012-10-12  \n",
      "3416  Massachusetts    1001.0      1476.0     1986-11-20  \n",
      "3423  Massachusetts    1001.0      1968.0     2008-09-19  \n",
      "3430  Massachusetts    1002.0      1607.0     2005-07-25  \n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "earthquake = pd.read_csv('data/Eartquakes-1990-2023.csv')\n",
    "realtor = pd.read_csv('data/realtor-data.csv')\n",
    "\n",
    "#Clean up empty spaces\n",
    "earthquake['state'] = earthquake['state'].str.strip()\n",
    "\n",
    "#Change USA to the correct state\n",
    "earthquake['state'] = earthquake['state'].replace('USA', 'Georgia')\n",
    "\n",
    "# US states \n",
    "states = us.states.STATES\n",
    "List = []\n",
    "for state in states:\n",
    "    List.append(state.name)\n",
    "    List.append(state.abbr)\n",
    "\n",
    "#Filter for US states \n",
    "filtered_earthquake = earthquake[earthquake['state'].isin(List)]\n",
    "filtered_realtor = realtor[realtor['state'].isin(List)]\n",
    "\n",
    "#Change all abbreviation to full name (only for earthquake)\n",
    "    # Dictionary mapping abbreviations to full names\n",
    "us_states = {state.abbr: state.name for state in states}\n",
    "filtered_earthquake.loc[:, 'state'] = filtered_earthquake['state'].apply(lambda x: us_states.get(x, x))\n",
    "\n",
    "#Drop NaN from both dataframe\n",
    "filtered_earthquake = filtered_earthquake.dropna()\n",
    "filtered_realtor = filtered_realtor.dropna()\n",
    "\n",
    "print(filtered_earthquake.head())\n",
    "print(filtered_realtor.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c185131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Chargement des comtés US (shapefile local)\n",
    "counties = gpd.read_file(\"data/cb_2018_us_county_500k/cb_2018_us_county_500k.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6df74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en GeoDataFrame\n",
    "gdf_eq = gpd.GeoDataFrame(\n",
    "    filtered_earthquake,\n",
    "    geometry=gpd.points_from_xy(filtered_earthquake.longitude, filtered_earthquake.latitude),\n",
    "    crs=counties.crs\n",
    ")\n",
    "\n",
    "# Jointure spatiale pour récupérer le county\n",
    "gdf_eq_with_county = gpd.sjoin(gdf_eq, counties, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Ajouter la colonne county dans le DataFrame original\n",
    "filtered_earthquake[\"county\"] = gdf_eq_with_county[\"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922fc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert timestamp (ms → seconds → datetime)\n",
    "filtered_earthquake[\"datetime\"] = pd.to_datetime(\n",
    "    filtered_earthquake[\"time\"] / 1000, unit=\"ns\"\n",
    ")\n",
    "\n",
    "# 2. Extract fields\n",
    "filtered_earthquake[\"year\"]  = filtered_earthquake[\"datetime\"].dt.year\n",
    "filtered_earthquake[\"month\"] = filtered_earthquake[\"datetime\"].dt.month\n",
    "filtered_earthquake[\"day\"]   = filtered_earthquake[\"datetime\"].dt.day\n",
    "\n",
    "# 3. Drop the old date column\n",
    "if \"date\" in filtered_earthquake.columns:\n",
    "    filtered_earthquake = filtered_earthquake.drop(columns=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fb8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger ton CSV avec les villes et comtés\n",
    "city_county = pd.read_csv(\"data/city_county.csv\")\n",
    "\n",
    "# Vérifie les colonnes pour être sûr\n",
    "# print(city_county.head())\n",
    "\n",
    "# Faire la jointure sur la colonne city\n",
    "# Assure-toi que les noms de colonnes correspondent (ici \"CITY\" dans city_county et \"city\" dans filtered_realtor)\n",
    "filtered_realtor = filtered_realtor.merge(\n",
    "    city_county[['CITY','COUNTY']],  # colonnes à joindre\n",
    "    left_on='city',                  # colonne dans filtered_realtor\n",
    "    right_on='CITY',                 # colonne dans city_county\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Renommer la colonne COUNTy si nécessaire\n",
    "filtered_realtor = filtered_realtor.rename(columns={'COUNTY':'county'})\n",
    "\n",
    "# Supprimer la colonne CITY doublon créée par le merge\n",
    "filtered_realtor = filtered_realtor.drop(columns=['CITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0e0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prev_sold_date to datetime\n",
    "filtered_realtor[\"prev_sold_date\"] = pd.to_datetime(\n",
    "    filtered_realtor[\"prev_sold_date\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Extract year / month / day\n",
    "filtered_realtor[\"sold_year\"] = filtered_realtor[\"prev_sold_date\"].dt.year\n",
    "filtered_realtor[\"sold_month\"] = filtered_realtor[\"prev_sold_date\"].dt.month\n",
    "filtered_realtor[\"sold_day\"] = filtered_realtor[\"prev_sold_date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d053c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Files saved: earthquake_cleaned.csv, realtor_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned versions without overwriting originals\n",
    "filtered_earthquake.to_csv('data/earthquake_cleaned.csv', index=False)\n",
    "filtered_realtor.to_csv('data/realtor_cleaned.csv', index=False)\n",
    "\n",
    "print(\"✔ Files saved: earthquake_cleaned.csv, realtor_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99011a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time                                 place     status  \\\n",
      "0         631153353990     12 km NNW of Meadow Lakes, Alaska   reviewed   \n",
      "1         631153491210            14 km S of Volcano, Hawaii   reviewed   \n",
      "2         631154083450            7 km W of Cobb, California   reviewed   \n",
      "3         631155512130  11 km E of Mammoth Lakes, California   reviewed   \n",
      "4         631155824490                16km N of Fillmore, CA   reviewed   \n",
      "...                ...                                   ...        ...   \n",
      "3445744  1690626699102           87 km NNW of Karluk, Alaska  automatic   \n",
      "3445745  1690626815980         0 km SW of Universal City, CA  automatic   \n",
      "3445747  1690626975715          Kodiak Island region, Alaska  automatic   \n",
      "3445749  1690628146040                    7 km W of Cobb, CA  automatic   \n",
      "3445750  1690628937884             35 km W of Karluk, Alaska  automatic   \n",
      "\n",
      "         tsunami  significance   data_type  magnitudo       state   longitude  \\\n",
      "0              0            96  earthquake       2.50      Alaska -149.669200   \n",
      "1              0            31  earthquake       1.41      Hawaii -155.212333   \n",
      "2              0            19  earthquake       1.11  California -122.806167   \n",
      "3              0            15  earthquake       0.98  California -118.846333   \n",
      "4              0           134  earthquake       2.95  California -118.934000   \n",
      "...          ...           ...         ...        ...         ...         ...   \n",
      "3445744        0            15  earthquake       1.00      Alaska -155.204500   \n",
      "3445745        0            16  earthquake       1.03  California -118.356833   \n",
      "3445747        0            44  earthquake       1.70      Alaska -153.729900   \n",
      "3445749        0            16  earthquake       1.03  California -122.800499   \n",
      "3445750        0            12  earthquake       0.90      Alaska -155.051000   \n",
      "\n",
      "          latitude    depth              county                      datetime  \\\n",
      "0        61.730200   30.100   Matanuska-Susitna 1970-01-01 00:00:00.631153353   \n",
      "1        19.317667    6.585              Hawaii 1970-01-01 00:00:00.631153491   \n",
      "2        38.821000    3.220              Sonoma 1970-01-01 00:00:00.631154083   \n",
      "3        37.664333   -0.584                Mono 1970-01-01 00:00:00.631155512   \n",
      "4        34.546000   16.122             Ventura 1970-01-01 00:00:00.631155824   \n",
      "...            ...      ...                 ...                           ...   \n",
      "3445744  58.241300    0.000  Lake and Peninsula 1970-01-01 00:00:01.690626699   \n",
      "3445745  34.135500   15.710         Los Angeles 1970-01-01 00:00:01.690626815   \n",
      "3445747  57.790100   24.400       Kodiak Island 1970-01-01 00:00:01.690626975   \n",
      "3445749  38.827499    1.720              Sonoma 1970-01-01 00:00:01.690628146   \n",
      "3445750  57.564800  250.000                 NaN 1970-01-01 00:00:01.690628937   \n",
      "\n",
      "         year  month  day  \n",
      "0        1970      1    1  \n",
      "1        1970      1    1  \n",
      "2        1970      1    1  \n",
      "3        1970      1    1  \n",
      "4        1970      1    1  \n",
      "...       ...    ...  ...  \n",
      "3445744  1970      1    1  \n",
      "3445745  1970      1    1  \n",
      "3445747  1970      1    1  \n",
      "3445749  1970      1    1  \n",
      "3445750  1970      1    1  \n",
      "\n",
      "[2742919 rows x 16 columns]          brokered_by    status     price  bed  bath  acre_lot     street  \\\n",
      "0            21163.0  for_sale  525000.0  3.0   3.0      0.45  1813270.0   \n",
      "1            67455.0  for_sale  289900.0  3.0   2.0      0.36  1698080.0   \n",
      "2            97400.0  for_sale  384900.0  3.0   2.0      0.46  1244899.0   \n",
      "3            33714.0  for_sale  199999.0  3.0   2.0      1.76  1745924.0   \n",
      "4            22188.0  for_sale  419000.0  4.0   2.0      2.00  1417448.0   \n",
      "...              ...       ...       ...  ...   ...       ...        ...   \n",
      "4792460     108243.0      sold  580000.0  5.0   3.0      0.31   307704.0   \n",
      "4792461     108243.0      sold  580000.0  5.0   3.0      0.31   307704.0   \n",
      "4792462     108243.0      sold  580000.0  5.0   3.0      0.31   307704.0   \n",
      "4792463     108243.0      sold  580000.0  5.0   3.0      0.31   307704.0   \n",
      "4792464     108243.0      sold  580000.0  5.0   3.0      0.31   307704.0   \n",
      "\n",
      "             city          state  zip_code  house_size prev_sold_date  \\\n",
      "0          Agawam  Massachusetts    1001.0      2314.0     2014-06-25   \n",
      "1          Agawam  Massachusetts    1001.0      1276.0     2012-10-12   \n",
      "2          Agawam  Massachusetts    1001.0      1476.0     1986-11-20   \n",
      "3          Agawam  Massachusetts    1001.0      1968.0     2008-09-19   \n",
      "4          Pelham  Massachusetts    1002.0      1607.0     2005-07-25   \n",
      "...           ...            ...       ...         ...            ...   \n",
      "4792460  Richland     Washington   99354.0      3615.0     2022-03-23   \n",
      "4792461  Richland     Washington   99354.0      3615.0     2022-03-23   \n",
      "4792462  Richland     Washington   99354.0      3615.0     2022-03-23   \n",
      "4792463  Richland     Washington   99354.0      3615.0     2022-03-23   \n",
      "4792464  Richland     Washington   99354.0      3615.0     2022-03-23   \n",
      "\n",
      "          county  sold_year  sold_month  sold_day  \n",
      "0        Hampden       2014           6        25  \n",
      "1        Hampden       2012          10        12  \n",
      "2        Hampden       1986          11        20  \n",
      "3        Hampden       2008           9        19  \n",
      "4         Shelby       2005           7        25  \n",
      "...          ...        ...         ...       ...  \n",
      "4792460    Baker       2022           3        23  \n",
      "4792461  Lebanon       2022           3        23  \n",
      "4792462   Oconee       2022           3        23  \n",
      "4792463  Navarro       2022           3        23  \n",
      "4792464   Benton       2022           3        23  \n",
      "\n",
      "[4792465 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_earthquake, filtered_realtor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff4b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved county + year aggregation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>year</th>\n",
       "      <th>n_earthquakes</th>\n",
       "      <th>avg_magnitude</th>\n",
       "      <th>max_magnitude</th>\n",
       "      <th>avg_depth</th>\n",
       "      <th>n_properties</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>median_price</th>\n",
       "      <th>avg_bedrooms</th>\n",
       "      <th>avg_bathrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>1970</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>2003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>432500.0</td>\n",
       "      <td>432500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44700.0</td>\n",
       "      <td>44700.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>264950.0</td>\n",
       "      <td>264950.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      county  year  n_earthquakes  avg_magnitude  max_magnitude  avg_depth  \\\n",
       "0  Abbeville  1970            5.0           2.12            2.4      2.914   \n",
       "1  Abbeville  2003            NaN            NaN            NaN        NaN   \n",
       "2  Abbeville  2004            NaN            NaN            NaN        NaN   \n",
       "3  Abbeville  2005            NaN            NaN            NaN        NaN   \n",
       "4  Abbeville  2006            NaN            NaN            NaN        NaN   \n",
       "\n",
       "   n_properties  avg_price  median_price  avg_bedrooms  avg_bathrooms  \n",
       "0           NaN        NaN           NaN           NaN            NaN  \n",
       "1           1.0   432500.0      432500.0           3.0            2.0  \n",
       "2           2.0    44700.0       44700.0           1.5            1.5  \n",
       "3           2.0   264950.0      264950.0           3.5            2.0  \n",
       "4           2.0   128900.0      128900.0           2.5            2.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Earthquake aggregation by county + year ---\n",
    "agg_eq_county_year = (\n",
    "    filtered_earthquake\n",
    "    .groupby(['county', 'year'])\n",
    "    .agg(\n",
    "        n_earthquakes=('magnitudo', 'count'),\n",
    "        avg_magnitude=('magnitudo', 'mean'),\n",
    "        max_magnitude=('magnitudo', 'max'),\n",
    "        avg_depth=('depth', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Realtor aggregation by county + year ---\n",
    "agg_re_county_year = (\n",
    "    filtered_realtor\n",
    "    .groupby(['county', 'sold_year'])\n",
    "    .agg(\n",
    "        n_properties=('price', 'count'),\n",
    "        avg_price=('price', 'mean'),\n",
    "        median_price=('price', 'median'),\n",
    "        avg_bedrooms=('bed', 'mean'),\n",
    "        avg_bathrooms=('bath', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'sold_year': 'year'})\n",
    ")\n",
    "\n",
    "# --- Combine both ---\n",
    "agg_county_year = agg_eq_county_year.merge(\n",
    "    agg_re_county_year,\n",
    "    on=['county', 'year'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "agg_county_year.to_csv(\"data/agg_county_year.csv\", index=False)\n",
    "print(\"✔ Saved county + year aggregation\")\n",
    "agg_county_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997c3b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Travail/Berne/Cours/TSM_InfVis/Module_Task/earthquake-realestate-viz/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m re = filtered_realtor.copy()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Extract year from earthquake timestamps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m eq[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(\u001b[43meq\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m eq[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] = eq[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].dt.year\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Realtor dataset may not have a meaningful year → optional\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# re[\"year\"] = pd.to_datetime(re[\"prev_sold_date\"], errors=\"coerce\").dt.year\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 2. EARTHQUAKE AGGREGATION (STATE)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Travail/Berne/Cours/TSM_InfVis/Module_Task/earthquake-realestate-viz/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Travail/Berne/Cours/TSM_InfVis/Module_Task/earthquake-realestate-viz/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. CLEAN + PREPARE DATA\n",
    "# ---------------------------\n",
    "\n",
    "eq = filtered_earthquake.copy()\n",
    "re = filtered_realtor.copy()\n",
    "\n",
    "# Extract year from earthquake timestamps\n",
    "eq[\"date\"] = pd.to_datetime(eq[\"date\"], errors=\"coerce\")\n",
    "eq[\"year\"] = eq[\"date\"].dt.year\n",
    "\n",
    "# Realtor dataset may not have a meaningful year → optional\n",
    "# re[\"year\"] = pd.to_datetime(re[\"prev_sold_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. EARTHQUAKE AGGREGATION (STATE)\n",
    "# ---------------------------\n",
    "\n",
    "agg_eq_state = (\n",
    "    eq.groupby(\"state\")\n",
    "      .agg(\n",
    "          n_earthquakes=(\"magnitudo\", \"count\"),\n",
    "          avg_magnitude=(\"magnitudo\", \"mean\"),\n",
    "          max_magnitude=(\"magnitudo\", \"max\"),\n",
    "          avg_depth=(\"depth\", \"mean\"),\n",
    "          first_quake=(\"year\", \"min\"),\n",
    "          last_quake=(\"year\", \"max\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Add an earthquake intensity score (useful for correlation later)\n",
    "agg_eq_state[\"intensity_score\"] = (\n",
    "    agg_eq_state[\"avg_magnitude\"] * 0.6 +\n",
    "    agg_eq_state[\"max_magnitude\"] * 0.4\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. REALTOR AGGREGATION (STATE)\n",
    "# ---------------------------\n",
    "\n",
    "agg_re_state = (\n",
    "    re.groupby(\"state\")\n",
    "      .agg(\n",
    "          n_properties=(\"price\", \"count\"),\n",
    "          avg_price=(\"price\", \"mean\"),\n",
    "          median_price=(\"price\", \"median\"),\n",
    "          avg_bedrooms=(\"bed\", \"mean\"),\n",
    "          avg_bathrooms=(\"bath\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Additional useful real-estate analytics\n",
    "agg_re_state[\"price_per_bedroom\"] = agg_re_state[\"avg_price\"] / agg_re_state[\"avg_bedrooms\"]\n",
    "agg_re_state[\"price_per_bathroom\"] = agg_re_state[\"avg_price\"] / agg_re_state[\"avg_bathrooms\"]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. MERGE BOTH AGGREGATIONS\n",
    "# ---------------------------\n",
    "\n",
    "agg_combined_state = agg_eq_state.merge(\n",
    "    agg_re_state, on=\"state\", how=\"outer\"\n",
    ")\n",
    "\n",
    "# Earthquake/property density ratio\n",
    "agg_combined_state[\"eq_per_100_properties\"] = (\n",
    "    agg_combined_state[\"n_earthquakes\"] /\n",
    "    (agg_combined_state[\"n_properties\"] / 100).replace({0: None})\n",
    ")\n",
    "\n",
    "# Sort by intensity or by price\n",
    "agg_combined_state = agg_combined_state.sort_values(\n",
    "    by=\"intensity_score\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"✔ Improved Aggregation by state completed:\")\n",
    "print(agg_combined_state.head())\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. SAVE\n",
    "# ---------------------------\n",
    "\n",
    "agg_combined_state.to_csv(\"data/state_aggregation.csv\", index=False)\n",
    "print(\"✔ Saved improved aggregation → data/state_aggregation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3115687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: data/agg_county_year.csv\n",
      "✔ Saved: data/agg_state_year.csv\n",
      "✔ County+Year sample:\n",
      "               county  year  n_earthquakes  avg_magnitude  max_magnitude  \\\n",
      "58664      Sweetwater  1995            1.0       5.180000           5.18   \n",
      "704    Aleutians West  1991            3.0       4.966667           5.40   \n",
      "61046      Tuscaloosa  1999            1.0       4.800000           4.80   \n",
      "705    Aleutians West  1993            6.0       4.583333           5.10   \n",
      "706    Aleutians West  1994            3.0       4.433333           5.00   \n",
      "\n",
      "       avg_depth  intensity_score  n_properties  avg_price  median_price  \\\n",
      "58664  -1.400000             5.18           NaN        NaN           NaN   \n",
      "704    58.900000             5.14           NaN        NaN           NaN   \n",
      "61046   1.000000             4.80           1.0   285000.0      285000.0   \n",
      "705    34.633333             4.79           NaN        NaN           NaN   \n",
      "706    30.966667             4.66           NaN        NaN           NaN   \n",
      "\n",
      "       avg_bedrooms  avg_bathrooms  price_per_bedroom  price_per_bathroom  \\\n",
      "58664           NaN            NaN                NaN                 NaN   \n",
      "704             NaN            NaN                NaN                 NaN   \n",
      "61046           3.0            2.0            95000.0            142500.0   \n",
      "705             NaN            NaN                NaN                 NaN   \n",
      "706             NaN            NaN                NaN                 NaN   \n",
      "\n",
      "       eq_per_100_properties  \n",
      "58664                    NaN  \n",
      "704                      NaN  \n",
      "61046                  100.0  \n",
      "705                      NaN  \n",
      "706                      NaN  \n",
      "✔ State+Year sample:\n",
      "              state  year  n_earthquakes  avg_magnitude  max_magnitude  \\\n",
      "888       Louisiana  2006            1.0       5.300000            5.3   \n",
      "509         Georgia  1991           76.0       4.023684            7.0   \n",
      "1045  Massachusetts  1992            1.0       4.800000            4.8   \n",
      "698         Indiana  2002            1.0       4.600000            4.6   \n",
      "60           Alaska  1996         5138.0       2.369093            7.9   \n",
      "\n",
      "      avg_depth  intensity_score  n_properties     avg_price  median_price  \\\n",
      "888    5.000000         5.300000         539.0  2.907727e+05      235000.0   \n",
      "509   11.725000         5.214211         189.0  5.732703e+05      450000.0   \n",
      "1045  10.000000         4.800000         481.0  1.387046e+06      724900.0   \n",
      "698   16.050000         4.600000         100.0  3.208720e+05      257450.0   \n",
      "60    39.403387         4.581456           NaN           NaN           NaN   \n",
      "\n",
      "      avg_bedrooms  avg_bathrooms  price_per_bedroom  price_per_bathroom  \\\n",
      "888       3.534323       2.476809       82271.128084       117398.126592   \n",
      "509       3.592593       2.798942      159570.094256       204816.812854   \n",
      "1045      3.893971       2.993763      356203.604912       463312.050000   \n",
      "698       3.180000       2.350000      100903.144654       136541.276596   \n",
      "60             NaN            NaN                NaN                 NaN   \n",
      "\n",
      "      eq_per_100_properties  \n",
      "888                0.185529  \n",
      "509               40.211640  \n",
      "1045               0.207900  \n",
      "698                1.000000  \n",
      "60                      NaN  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. CLEAN + PREPARE DATA\n",
    "# ---------------------------\n",
    "\n",
    "eq = filtered_earthquake.copy()\n",
    "re = filtered_realtor.copy()\n",
    "\n",
    "# Convert timestamps → datetime\n",
    "eq[\"datetime\"] = pd.to_datetime(eq[\"time\"], unit=\"ms\", errors=\"coerce\")\n",
    "eq[\"year\"] = eq[\"datetime\"].dt.year\n",
    "\n",
    "# Realtor sold date → datetime\n",
    "re[\"sold_date\"] = pd.to_datetime(re[\"prev_sold_date\"], errors=\"coerce\")\n",
    "re[\"sold_year\"] = re[\"sold_date\"].dt.year\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ==========     EARTHQUAKE AGGREGATIONS     ===========\n",
    "# ======================================================\n",
    "\n",
    "# ---------- EQ by county + year ----------\n",
    "agg_eq_county_year = (\n",
    "    eq.groupby([\"county\", \"year\"])\n",
    "      .agg(\n",
    "          n_earthquakes=(\"magnitudo\", \"count\"),\n",
    "          avg_magnitude=(\"magnitudo\", \"mean\"),\n",
    "          max_magnitude=(\"magnitudo\", \"max\"),\n",
    "          avg_depth=(\"depth\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# ---------- EQ by state + year ----------\n",
    "agg_eq_state_year = (\n",
    "    eq.groupby([\"state\", \"year\"])\n",
    "      .agg(\n",
    "          n_earthquakes=(\"magnitudo\", \"count\"),\n",
    "          avg_magnitude=(\"magnitudo\", \"mean\"),\n",
    "          max_magnitude=(\"magnitudo\", \"max\"),\n",
    "          avg_depth=(\"depth\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Add intensity score useful for ranking\n",
    "for df in [agg_eq_county_year, agg_eq_state_year]:\n",
    "    df[\"intensity_score\"] = (\n",
    "        df[\"avg_magnitude\"] * 0.6 +\n",
    "        df[\"max_magnitude\"] * 0.4\n",
    "    )\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ==========      REALTOR AGGREGATIONS      ============\n",
    "# ======================================================\n",
    "\n",
    "# ---------- RE by county + year ----------\n",
    "agg_re_county_year = (\n",
    "    re.groupby([\"county\", \"sold_year\"])\n",
    "      .agg(\n",
    "          n_properties=(\"price\", \"count\"),\n",
    "          avg_price=(\"price\", \"mean\"),\n",
    "          median_price=(\"price\", \"median\"),\n",
    "          avg_bedrooms=(\"bed\", \"mean\"),\n",
    "          avg_bathrooms=(\"bath\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .rename(columns={\"sold_year\": \"year\"})\n",
    ")\n",
    "\n",
    "# ---------- RE by state + year ----------\n",
    "agg_re_state_year = (\n",
    "    re.groupby([\"state\", \"sold_year\"])\n",
    "      .agg(\n",
    "          n_properties=(\"price\", \"count\"),\n",
    "          avg_price=(\"price\", \"mean\"),\n",
    "          median_price=(\"price\", \"median\"),\n",
    "          avg_bedrooms=(\"bed\", \"mean\"),\n",
    "          avg_bathrooms=(\"bath\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .rename(columns={\"sold_year\": \"year\"})\n",
    ")\n",
    "\n",
    "for df in [agg_re_county_year, agg_re_state_year]:\n",
    "    df[\"price_per_bedroom\"] = df[\"avg_price\"] / df[\"avg_bedrooms\"]\n",
    "    df[\"price_per_bathroom\"] = df[\"avg_price\"] / df[\"avg_bathrooms\"]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ==========         MERGE AGGREGATIONS     ============\n",
    "# ======================================================\n",
    "\n",
    "# ---------- COUNTY + YEAR ----------\n",
    "agg_county_year = agg_eq_county_year.merge(\n",
    "    agg_re_county_year,\n",
    "    on=[\"county\", \"year\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "agg_county_year[\"eq_per_100_properties\"] = (\n",
    "    agg_county_year[\"n_earthquakes\"] /\n",
    "    (agg_county_year[\"n_properties\"] / 100).replace({0: None})\n",
    ")\n",
    "\n",
    "agg_county_year = agg_county_year.sort_values(\n",
    "    by=\"intensity_score\", ascending=False\n",
    ")\n",
    "\n",
    "# ---------- STATE + YEAR ----------\n",
    "agg_state_year = agg_eq_state_year.merge(\n",
    "    agg_re_state_year,\n",
    "    on=[\"state\", \"year\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "agg_state_year[\"eq_per_100_properties\"] = (\n",
    "    agg_state_year[\"n_earthquakes\"] /\n",
    "    (agg_state_year[\"n_properties\"] / 100).replace({0: None})\n",
    ")\n",
    "\n",
    "agg_state_year = agg_state_year.sort_values(\n",
    "    by=\"intensity_score\", ascending=False\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. SAVE RESULTS\n",
    "# ---------------------------\n",
    "\n",
    "agg_county_year.to_csv(\"data/agg_county_year.csv\", index=False)\n",
    "agg_state_year.to_csv(\"data/agg_state_year.csv\", index=False)\n",
    "\n",
    "print(\"✔ Saved: data/agg_county_year.csv\")\n",
    "print(\"✔ Saved: data/agg_state_year.csv\")\n",
    "\n",
    "print(\"✔ County+Year sample:\")\n",
    "print(agg_county_year.head())\n",
    "\n",
    "print(\"✔ State+Year sample:\")\n",
    "print(agg_state_year.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake-realestate-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
